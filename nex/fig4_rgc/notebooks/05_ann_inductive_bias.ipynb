{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d511b8-02c5-489c-b389-9b355ee9e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import torch\n",
    "from torch import ones, zeros, eye, as_tensor, tensor, float32\n",
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../nex/rgc/utils/\")\n",
    "from data_utils import (\n",
    "    read_data,\n",
    "    build_avg_recordings,\n",
    "    build_training_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40e0f70-1411-4eb5-a5d7-ea7ac344a2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.4.0\n",
      "pandas 2.2.2\n",
      "numpy 1.26.4\n"
     ]
    }
   ],
   "source": [
    "print(\"torch\", torch.__version__)\n",
    "print(\"pandas\", pd.__version__)\n",
    "print(\"numpy\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b04dc8-6109-4569-b19e-8dc742249e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a472d4-150f-4205-86cd-715756a31c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rho(net, data, inds, loss_weights, labels):\n",
    "    predictions = net(data[inds])\n",
    "    labels_split = labels[inds]\n",
    "    loss_weights_split = torch.as_tensor(loss_weights[inds], dtype=torch.bool)\n",
    "\n",
    "    rhos = []\n",
    "    # Loop across all ROIs.\n",
    "    for i in range(predictions.shape[1]):\n",
    "        loss_weights_split_of_roi = loss_weights_split[:, i]\n",
    "        relevant_pred_roi = predictions[loss_weights_split_of_roi, i]\n",
    "        relevant_labels_roi = labels_split[loss_weights_split_of_roi, i]\n",
    "        rho_roi = np.corrcoef(relevant_pred_roi.detach().numpy(), relevant_labels_roi.numpy())[0, 1]\n",
    "        rhos.append(rho_roi)\n",
    "    rho2 = np.mean(rhos)\n",
    "\n",
    "    return rho2\n",
    "\n",
    "\n",
    "def eval_nn(net_, data, labels, loss_weights, seed, val_frac, num_test, verbose=False):\n",
    "    _ = torch.manual_seed(seed)\n",
    "\n",
    "    num_datapoints = len(data)\n",
    "    # Validation fraction is computed without considering the test set.\n",
    "    test_frac = num_test / num_datapoints\n",
    "    val_frac = val_frac * (1 - test_frac)\n",
    "    num_train = int(num_datapoints * (1 - val_frac - test_frac))\n",
    "    \n",
    "    num_splits = 1\n",
    "    train_inds = []\n",
    "    val_inds = []\n",
    "    test_inds = []\n",
    "    for i in range(num_splits):\n",
    "        permutation = torch.randperm(num_datapoints)\n",
    "        # Perform data splits.\n",
    "        test_inds.append(permutation[:num_test])\n",
    "        train_inds.append(permutation[num_test:num_test+num_train])\n",
    "        val_inds.append(permutation[num_test+num_train:])\n",
    "\n",
    "    cross_val_test_accuracies = []\n",
    "    cross_val_val_accuracies = []\n",
    "    cross_val_train_accuracies = []\n",
    "    for split in range(num_splits):\n",
    "        net = deepcopy(net_)\n",
    "        optimizer = Adam(list(net.parameters()), lr=1e-3)\n",
    "    \n",
    "        best_rho = -20.0\n",
    "        \n",
    "        for epoch in range(500):\n",
    "            optimizer.zero_grad()\n",
    "            predictions = net(data[train_inds[split]])\n",
    "            diff = (predictions - labels[train_inds[split]])**2\n",
    "            loss = diff * loss_weights[train_inds[split]]\n",
    "            loss = torch.mean(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Evaluation\n",
    "            if epoch % 10 == 0:\n",
    "                rho = eval_rho(net, data, val_inds[split], loss_weights, labels)\n",
    "\n",
    "                if rho > best_rho:\n",
    "                    best_rho = rho\n",
    "                    best_rho_train = eval_rho(net, data, train_inds[split], loss_weights, labels)\n",
    "                    best_rho_test = eval_rho(net, data, test_inds[split], loss_weights, labels)\n",
    "            \n",
    "        cross_val_test_accuracies.append(best_rho_test)\n",
    "        cross_val_train_accuracies.append(best_rho_train)\n",
    "        cross_val_val_accuracies.append(best_rho)\n",
    "        \n",
    "    return np.mean(cross_val_test_accuracies), np.mean(cross_val_train_accuracies), np.mean(cross_val_val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d607d4af-df37-4625-a194-8411ee805be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_accuracies = {}\n",
    "all_linreg_accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba6f3c4b-4b1a-4c5b-abca-4bbd3d18b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_recordings_each_scanfield [15]\n",
      "number_of_recordings_each_scanfield [15]\n",
      "number_of_recordings_each_scanfield [15]\n",
      "number_of_recordings_each_scanfield [15]\n",
      "number_of_recordings_each_scanfield [15]\n"
     ]
    }
   ],
   "source": [
    "################## RECORDINGS ##################\n",
    "linreg_accuracies = {\"train\": [], \"test\": []}\n",
    "ann_accuracies = {\"train\": [], \"test\": []}\n",
    "myrec = 3  # 1,2,3,5,7,9,13\n",
    "\n",
    "for n_train in [32,  64, 128, 256, 512]:\n",
    "    test_num = 512\n",
    "    start_n_scan = 0\n",
    "\n",
    "    num_datapoints_per_scanfield = test_num + n_train\n",
    "    cell_id = \"20161028_1\"\n",
    "    rec_ids = [myrec]\n",
    "    nseg = 4\n",
    "    \n",
    "    stimuli, recordings, setup, noise_full = read_data(\n",
    "        start_n_scan,\n",
    "        num_datapoints_per_scanfield,\n",
    "        cell_id,\n",
    "        rec_ids,\n",
    "        \"noise\",\n",
    "        \"..\"\n",
    "    )\n",
    "    \n",
    "    avg_recordings = build_avg_recordings(\n",
    "        recordings, rec_ids, nseg, num_datapoints_per_scanfield\n",
    "    )\n",
    "    \n",
    "    ################## DATASET ##################\n",
    "    number_of_recordings_each_scanfield = list(avg_recordings.groupby(\"rec_id\").size())\n",
    "    print(f\"number_of_recordings_each_scanfield {number_of_recordings_each_scanfield}\")\n",
    "    number_of_recordings = np.sum(number_of_recordings_each_scanfield)\n",
    "    assert len(number_of_recordings_each_scanfield) == len(rec_ids)\n",
    "    \n",
    "    # Back to ANN code\n",
    "    linears = []\n",
    "    mlps = []\n",
    "    \n",
    "    linears_train = []\n",
    "    mlps_train = []\n",
    "    n_out = number_of_recordings\n",
    "    \n",
    "    warmup = 5.0\n",
    "    i_amp = 0.1\n",
    "    \n",
    "    _, labels, loss_weights = build_training_data(\n",
    "        i_amp,\n",
    "        stimuli,\n",
    "        avg_recordings,\n",
    "        rec_ids,\n",
    "        num_datapoints_per_scanfield,\n",
    "        number_of_recordings_each_scanfield,\n",
    "    )\n",
    "    \n",
    "    data_global = torch.as_tensor(np.reshape(noise_full, (300, noise_full.shape[2])).T)\n",
    "    labels_global = torch.as_tensor(labels)\n",
    "    loss_weights = torch.as_tensor(loss_weights)\n",
    "    \n",
    "    ################## TRAINING ##################\n",
    "    val_frac = 0.2\n",
    "\n",
    "    linreg = {\"train\": [], \"test\": []}\n",
    "    ann = {\"train\": [], \"test\": []}\n",
    "    for seed in range(5):\n",
    "        _ = torch.manual_seed(seed)\n",
    "        net = nn.Linear(300, n_out)\n",
    "        linears, linears_train, linears_val = eval_nn(net, data_global, labels_global, loss_weights, seed+1, val_frac, test_num, False)\n",
    "        # Average across splits (although currently only one is used).\n",
    "        linreg[\"train\"].append(np.mean(linears_train))\n",
    "        linreg[\"test\"].append(np.mean(linears))\n",
    "        \n",
    "        _ = torch.manual_seed(seed)\n",
    "        net = nn.Sequential(\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, n_out)\n",
    "        )\n",
    "        mlps, mlps_train, mlps_val = eval_nn(net, data_global, labels_global, loss_weights, seed+1, val_frac, test_num, False)\n",
    "        # Average across splits (although currently only one is used).\n",
    "        ann[\"train\"].append(np.mean(mlps_train))\n",
    "        ann[\"test\"].append(np.mean(mlps))\n",
    "\n",
    "    # Take average across all seeds.\n",
    "    linreg_accuracies[\"train\"].append(np.mean(linreg[\"train\"]))\n",
    "    linreg_accuracies[\"test\"].append(np.mean(linreg[\"test\"]))\n",
    "\n",
    "    ann_accuracies[\"train\"].append(np.mean(ann[\"train\"]))\n",
    "    ann_accuracies[\"test\"].append(np.mean(ann[\"test\"]))\n",
    "\n",
    "all_ann_accuracies[myrec] = ann_accuracies\n",
    "all_linreg_accuracies[myrec] = linreg_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c1d5ed1-bc44-49f0-bd11-1bae3a7520ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../results/05_ann_inductive_bias/linreg_accuracies.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(all_linreg_accuracies, handle)\n",
    "\n",
    "with open(f\"../results/05_ann_inductive_bias/ann_accuracies.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(all_ann_accuracies, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472f848-e469-478a-b899-832d863d82cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
